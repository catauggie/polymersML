{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9847ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "poly_data = pd.read_excel('resulting_dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df1a3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_data_cols_medians = [a for a in poly_data.columns if 'median' in a]\n",
    "poly_data_cols_vars = [a for a in poly_data.columns if 'variance' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa28c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('cols.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6b79cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_data = pd.read_excel('merged_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e96d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "bests = pd.read_excel('max_r2.xlsx')\n",
    "best_chars = list(bests['Charactristic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a75a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваши листы данных\n",
    "all_cols = best_chars\n",
    "parts_list = ['Hansen parameter delta-h: hydrogen bonding', 'Hansen parameter delta-d: dispersion component'] \n",
    "\n",
    "# Находим названия, которые есть в all_cols, но отсутствуют в parts_list\n",
    "missing_names = set(all_cols) - set(parts_list)\n",
    "\n",
    "# Преобразуем результат обратно в список, если это необходимо\n",
    "missing_names_list = list(missing_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5e137fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = filled_data[missing_names_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cce158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with the suffix '_value_variance' and names from missing_names_list\n",
    "selected_columns = [col for col in poly_data.columns if any(name in col for name in missing_names_list) and col.endswith('_value_variance')]\n",
    "\n",
    "# Extract the relevant subset of the DataFrame\n",
    "subset_poly_data = poly_data[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c35b7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mains = poly_data[['polymer_name', 'SMILES']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ec45f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([mains, dt, subset_poly_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef317a13",
   "metadata": {},
   "source": [
    "## Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc26c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.read_excel('RESULT.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b19ea99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_cols = [col for col in result_df.columns if any(name in col for name in best_chars) and col.endswith('_value_median')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f77bca74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elongation at yield'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_cols[0].replace('_value_median', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "be46c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = result_df[result_df['Specific volume_value_variance'].isna() == True][selected_cols]\n",
    "test_df = result_df[result_df['Specific volume_value_variance'].isna() == False][selected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ebb2d581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        12.574191\n",
       "1         0.002383\n",
       "2         0.003568\n",
       "3         0.004569\n",
       "4         0.000092\n",
       "           ...    \n",
       "18219     0.000000\n",
       "18220     0.000000\n",
       "18221     0.000000\n",
       "18222     0.000000\n",
       "18309     0.000000\n",
       "Name: Specific volume_value_variance, Length: 1739, dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[result_df['Specific volume_value_variance'].isna() == False]['Specific volume_value_variance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "973b7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df.drop(['Specific volume_value_median'], axis=1), train_df['Specific volume_value_median']\n",
    "X_test, y_test = test_df.drop(['Specific volume_value_median'], axis=1), test_df['Specific volume_value_median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d71b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No samples available for training the model for column: Volume resistivity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ivan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.541e+05, tolerance: 2.864e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "D:\\Users\\ivan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.301e+03, tolerance: 3.653e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No samples available for training the model for column: Volume resistivity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ivan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "D:\\Users\\ivan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "D:\\Users\\ivan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+04, tolerance: 8.027e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "D:\\Users\\ivan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.847e+06, tolerance: 4.171e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No samples available for training the model for column: Volume resistivity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ivan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "D:\\Users\\ivan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.538e+07, tolerance: 1.623e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "D:\\Users\\ivan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "D:\\Users\\ivan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e+08, tolerance: 2.734e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No samples available for training the model for column: Volume resistivity\n",
      "No samples available for training the model for column: Volume resistivity\n",
      "No samples available for training the model for column: Volume resistivity\n",
      "No samples available for training the model for column: Volume resistivity\n",
      "No samples available for training the model for column: Volume resistivity\n",
      "No samples available for training the model for column: Volume resistivity\n",
      "No samples available for training the model for column: Volume resistivity\n",
      "No samples available for training the model for column: Volume resistivity\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.linear_model import  LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalized_mse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    var_y = np.var(y_true)\n",
    "    nmse = mse / var_y\n",
    "    return nmse\n",
    "\n",
    "def mean_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    non_zero_indices = y_true != 0\n",
    "    y_true_non_zero = y_true[non_zero_indices]\n",
    "    y_pred_non_zero = y_pred[non_zero_indices]\n",
    "    \n",
    "    # Calculate MPE\n",
    "    mpe = abs(np.mean((y_true_non_zero - y_pred_non_zero) / y_true_non_zero) * 100)\n",
    "    return mpe\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def custom_metric(row):\n",
    "    if row['var'] > 0:\n",
    "        return 1 if abs(row['y_pred'] - row['y_test']) < row['var'] else 0\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def run_knn(X_train, X_test, y_train, y_test):\n",
    "    model = KNeighborsRegressor(n_neighbors=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    coefs = None  # KNN Regressor doesn't have coefficients\n",
    "    return y_pred, coefs\n",
    "\n",
    "def run_lasso(X_train, X_test, y_train, y_test):\n",
    "    model = Lasso()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    coefs = model.coef_\n",
    "    return y_pred, coefs\n",
    "\n",
    "def run_elastic_net(X_train, X_test, y_train, y_test):\n",
    "    model = ElasticNet()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    coefs = model.coef_\n",
    "    return y_pred, coefs\n",
    "\n",
    "def run_decision_tree(X_train, X_test, y_train, y_test):\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    coefs = None  # Decision Tree doesn't have coefficients\n",
    "    return y_pred, coefs\n",
    "\n",
    "def run_bagging(X_train, X_test, y_train, y_test):\n",
    "    model = BaggingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    coefs = None  # Bagging doesn't have coefficients\n",
    "    return y_pred, coefs\n",
    "\n",
    "def run_adaboost(X_train, X_test, y_train, y_test):\n",
    "    model = AdaBoostRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    coefs = None  # AdaBoost doesn't have coefficients\n",
    "    return y_pred, coefs\n",
    "\n",
    "def run_xgboost(X_train, X_test, y_train, y_test):\n",
    "    model = XGBRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    coefs = None  # XGBoost doesn't have coefficients\n",
    "    return y_pred, coefs\n",
    "\n",
    "def run_svr(X_train, X_test, y_train, y_test):\n",
    "    model = SVR()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    coefs = None  # SVR doesn't have coefficients\n",
    "    return y_pred, coefs\n",
    "\n",
    "def run_gradient_boosting(X_train, X_test, y_train, y_test):\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    coefs = None  # GradientBoostingRegressor doesn't have coefficients\n",
    "    return y_pred, coefs\n",
    "\n",
    "def run_linear_regression(X_train, X_test, y_train, y_test):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    coefs = model.coef_\n",
    "    return y_pred, coefs\n",
    "\n",
    "def run_random_forest(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    coefs = model.feature_importances_\n",
    "    return y_pred, coefs\n",
    "\n",
    "\n",
    "# List of models and their names\n",
    "models = [run_knn, run_lasso, run_elastic_net, run_decision_tree, run_bagging, run_adaboost,\n",
    "          run_xgboost, run_svr, run_gradient_boosting, run_linear_regression, run_random_forest]\n",
    "model_names = ['KNeighborsRegressor', 'Lasso', 'Elastic Net', 'Decision Tree', 'Bagging', 'AdaBoost', 'XGBoost',\n",
    "               'SVR', 'Gradient Boosting', 'Linear Regression', 'Random Forest']\n",
    "\n",
    "\n",
    "# List of models and their names\n",
    "models = [run_knn]\n",
    "model_names = ['KNeighborsRegressor']\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "    \n",
    "    # Specify the directory and file name\n",
    "    model_directory = f'check/{model_name}'\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(model_directory, exist_ok=True)\n",
    "        \n",
    "    mse_list = []\n",
    "    r2_list = []\n",
    "    rmse_list = []\n",
    "    nmse_list = []\n",
    "    mae_list = []\n",
    "    mpe_list = []\n",
    "    overall_list = []\n",
    "    data_list = []\n",
    "    name_list = []\n",
    "\n",
    "    for q in range(len(selected_cols)):\n",
    "\n",
    "        cleaned_column_name = selected_cols[q].replace('_value_median', '')\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        train_df = result_df[result_df[f'{cleaned_column_name}_value_variance'].isna() == True][selected_cols]\n",
    "        test_df = result_df[result_df[f'{cleaned_column_name}_value_variance'].isna() == False][selected_cols]\n",
    "        \n",
    "        X_train, y_train = train_df.drop([f'{cleaned_column_name}_value_median'], axis=1), train_df[f'{cleaned_column_name}_value_median']\n",
    "        X_test, y_test = test_df.drop([f'{cleaned_column_name}_value_median'], axis=1), test_df[f'{cleaned_column_name}_value_median']\n",
    "        \n",
    "        \n",
    "        # Check if there are samples available before training the model\n",
    "        if X_train.shape[0] == 0:\n",
    "            print(f\"No samples available for training the model for column: {cleaned_column_name}\")\n",
    "            continue\n",
    "\n",
    "        data_size = X_test.shape[0]\n",
    "        \n",
    "        # Train the model on the training data\n",
    "        try:\n",
    "            y_pred, coefs = model(X_train, X_test, y_train, y_test)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error training the model for column {cleaned_column_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = root_mean_squared_error(y_test, y_pred)\n",
    "        nmse = normalized_mse(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mpe = mean_percentage_error(y_test, y_pred)\n",
    "\n",
    "        indexes_to_show = y_test.index\n",
    "        y_var = result_df[result_df[f'{cleaned_column_name}_value_variance'].isna() == False][f'{cleaned_column_name}_value_variance']\n",
    "        tst = list(y_test)\n",
    "        prd = list(y_pred)\n",
    "        ndf = pd.DataFrame({'y_test': tst, 'y_pred': prd, 'var': y_var})\n",
    "\n",
    "        # Apply the custom metric to each row\n",
    "        ndf['metric'] = ndf.apply(custom_metric, axis=1)\n",
    "\n",
    "        # Calculate the overall metric (excluding rows where var <= 0)\n",
    "        filtered_df = ndf.dropna(subset=['metric'])\n",
    "        overall = filtered_df['metric'].mean()\n",
    "\n",
    "        data_list.append(data_size)\n",
    "        name_list.append(cleaned_column_name)\n",
    "        mse_list.append(mse)\n",
    "        r2_list.append(r2)\n",
    "        rmse_list.append(rmse)\n",
    "        nmse_list.append(nmse)\n",
    "        mae_list.append(mae)\n",
    "        mpe_list.append(mpe)\n",
    "        overall_list.append(overall)\n",
    "\n",
    "    # Create a DataFrame for the current model\n",
    "    res_model = pd.DataFrame({\n",
    "        'Data Size': data_list,\n",
    "        'Name': name_list,\n",
    "        'MSE': mse_list,\n",
    "        'R2': r2_list,\n",
    "        'RMSE': rmse_list,\n",
    "        'NMSE': nmse_list,\n",
    "        'MAE': mae_list,\n",
    "        'MPE': mpe_list,\n",
    "        'Overall': overall_list\n",
    "    })\n",
    "\n",
    "    # Save the results to an Excel file\n",
    "    with pd.ExcelWriter(f'{model_directory}/results_{model_name}.xlsx', engine='openpyxl', mode='w') as writer:\n",
    "        res_model.to_excel(writer, sheet_name='Results', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def parse_all_xlsx_files(folder_path):\n",
    "    all_dfs = []\n",
    "\n",
    "    # Walk through the directory and its subdirectories\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Check if the file has a .xlsx extension\n",
    "            if file.endswith('.xlsx'):\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                # Try to read the Excel file into a DataFrame\n",
    "                try:\n",
    "                    df = pd.read_excel(file_path)\n",
    "                    \n",
    "                    # Store both the file name and the DataFrame in a tuple\n",
    "                    result = (file, df)\n",
    "                    all_dfs.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file}: {e}\")\n",
    "\n",
    "    return all_dfs\n",
    "\n",
    "# Specify the folder path containing Excel files\n",
    "folder_path = 'check2'\n",
    "\n",
    "# Call the function to parse all Excel files in the folder and its subfolders\n",
    "list_of_dfs = parse_all_xlsx_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a sequence of markers\n",
    "markers = ['o', '^', 's', 'D', 'v', '<', '>', 'p', '*', 'h']\n",
    "\n",
    "# Create an empty list to store legend handles and labels\n",
    "legend_handles = []\n",
    "\n",
    "# Assuming list_of_dfs is already populated\n",
    "for d in range(len(list_of_dfs)):\n",
    "    index_to_plot = d\n",
    "    \n",
    "    df_to_plot = list_of_dfs[index_to_plot][1]\n",
    "    file_name = list_of_dfs[index_to_plot][0]\n",
    "\n",
    "    # Split the file name by underscore and get the second part\n",
    "    entity = file_name.split('_')[1]\n",
    "\n",
    "    # Split the entity by dot and get the first part\n",
    "    parsed_entity = entity.split('.')[0]\n",
    "\n",
    "    # Set a range for the y-axis to avoid extreme values\n",
    "    y_axis_lower_limit = -0.05  # Set your desired lower limit\n",
    "    y_axis_upper_limit = 1.05   # Set your desired upper limit\n",
    "\n",
    "    # Filter values within the specified range\n",
    "    filtered_df = df_to_plot[(df_to_plot['Overall'] >= y_axis_lower_limit) & (df_to_plot['Overall'] <= y_axis_upper_limit)]\n",
    "\n",
    "    # Set 'Name' column as the index\n",
    "    filtered_df.set_index('Name', inplace=True)\n",
    "\n",
    "    # Get the marker for the current plot\n",
    "    current_marker = markers[d % len(markers)]\n",
    "\n",
    "    # Create a scatter plot using 'Name' as x-axis labels and the current marker\n",
    "    scatter = plt.scatter(filtered_df.index, filtered_df['Overall'], label=f'VM of {parsed_entity}', marker=current_marker)\n",
    "\n",
    "    # Add zebra-like background\n",
    "    for i, index_name in enumerate(filtered_df.index):\n",
    "        color = 'lightgray' if i % 2 == 0 else 'white'\n",
    "        # Adjust the span to cover the entire area between x-axis ticks\n",
    "        plt.axvspan(i - 0.5, i + 0.5, facecolor=color, alpha=0.3, zorder=-1)\n",
    "\n",
    "    # Append the scatter plot handle to the legend_handles list\n",
    "    legend_handles.append(scatter)\n",
    "\n",
    "# Draw horizontal red lines at y=0 and y=1\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=1, label='y=0')\n",
    "plt.axhline(1, color='red', linestyle='--', linewidth=1, label='y=1')\n",
    "\n",
    "# Set labels and title\n",
    "plt.title('Variance Metric (VM) score depending on the characteristic')\n",
    "plt.xlabel('Name of characteristic')\n",
    "plt.ylabel('Variance Metric score')\n",
    "\n",
    "# Rotate the figure 90 degrees clockwise\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add legend outside the plot\n",
    "plt.legend(handles=legend_handles, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Set a white background\n",
    "plt.gca().set_facecolor('white')\n",
    "\n",
    "# Adjust figure size if needed\n",
    "# Adjust figure size if needed\n",
    "plt.gcf().set_size_inches(12, 8)\n",
    "\n",
    "# Save the figure with an explicit bounding box\n",
    "plt.savefig('VM_all_checked.eps', bbox_inches='tight', format='eps', dpi=300)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
